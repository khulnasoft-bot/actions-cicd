{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as df\n",
    "from dask_ml.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from dask.distributed import Client\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Callable, List\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from dask import array as da\n",
    "from textacy.preprocess import preprocess_text\n",
    "import dask.multiprocessing\n",
    "from pathos.multiprocessing import cpu_count\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(os.getenv(\"DASK_SCHEDULER_ADDRESS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/dask/dataframe/core.py:2966: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('body', 'object'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n",
      "/usr/local/lib/python3.6/site-packages/dask/dataframe/core.py:2966: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('title', 'object'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "output_dir = \"/data/\"\n",
    "\n",
    "base_url = 'https://storage.googleapis.com/codenet/issue_labels/'\n",
    "dd = df.from_pandas(pd.concat([pd.read_csv(base_url+f'00000000000{i}.csv.gz') for i in range(1)]), npartitions=64)\n",
    "\n",
    "def textacy_cleaner(text: str) -> str:\n",
    "    \"\"\"a\n",
    "    Defines the default function for cleaning text.\n",
    "\n",
    "    This function operates over a list.\n",
    "    \"\"\"\n",
    "    return preprocess_text(text,\n",
    "                           lowercase=True,\n",
    "                           no_emails=True,\n",
    "                           no_phone_numbers=True,\n",
    "                           no_numbers=True,\n",
    "                           no_currency_symbols=True,\n",
    "                           no_punct=True,\n",
    "                           no_contractions=False,\n",
    "                           no_accents=True)\n",
    "\n",
    "\n",
    "def process_document(doc: str) -> List[str]:\n",
    "    if doc and len(doc) > 20000:\n",
    "        return [\"_start_\", \"\", \"_end_\"]\n",
    "    doc = text_to_word_sequence(textacy_cleaner(doc))\n",
    "    if len(doc) > 1000:\n",
    "        return [\"_start_\", \"\", \"_end_\"]\n",
    "    return [\"_start_\"] + doc + [\"_end_\"]\n",
    "\n",
    "\n",
    "test_data = 'hello world 314-903-3072, hamel.husain@gmail.com wee woo'\n",
    "assert process_document(test_data) == ['_start_', 'hello', 'world', 'phone', 'email', 'wee', 'woo', '_end_']\n",
    "\n",
    "\n",
    "bodies_parsed = dd[\"body\"].apply(process_document)\n",
    "titles_parsed = dd[\"title\"].apply(process_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/dask/dataframe/core.py:2966: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('body', 'int64'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized 8.895066499710083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/dask/dataframe/core.py:2966: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('title', 'int64'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantiles title-13 body-172 \n",
      "quantiles done 123.69262671470642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/dask/dataframe/core.py:2966: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('body', 'object'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n",
      "/usr/local/lib/python3.6/site-packages/dask/dataframe/core.py:2966: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('title', 'object'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    }
   ],
   "source": [
    "now = time.time() - start_time\n",
    "print(f\"tokenized {now}\")\n",
    "\n",
    "def to_one_hot(df):\n",
    "    return to_categorical(df.values, num_classes=3)\n",
    "\n",
    "targets = dd[\"class_int\"].to_frame().map_partitions(to_one_hot)\n",
    "\n",
    "body_quant = int(bodies_parsed.apply(len).quantile(q=0.85).compute())\n",
    "title_quant = int(titles_parsed.apply(len).quantile(q=0.85).compute())\n",
    "\n",
    "print(f\"Quantiles title-{title_quant} body-{body_quant} \")\n",
    "\n",
    "def drop_long_docs(doc, max_len):\n",
    "    if len(doc) > max_len:\n",
    "        return doc[:max_len]\n",
    "    return doc\n",
    "\n",
    "bodies_parsed = bodies_parsed.apply(drop_long_docs, max_len=body_quant)\n",
    "titles_parsed = titles_parsed.apply(drop_long_docs, max_len=title_quant)\n",
    "\n",
    "def count_words(partition):\n",
    "    c = Counter()\n",
    "    def count(p):\n",
    "        c.update(p)\n",
    "        return c\n",
    "    ct = Counter()\n",
    "    ct.update(dict(partition.apply(count).iloc[0].most_common(n=8000)))\n",
    "    return ct\n",
    "\n",
    "\n",
    "now = time.time() - start_time\n",
    "print(f\"quantiles done {now}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/distributed/worker.py:3217: UserWarning: Large object of size 3.73 MB detected in task graph: \n",
      "  ('apply-628d5b10edef41eb4b38b63939b9371c', <functi ... lumns], 'body')\n",
      "Consider scattering large objects ahead of time\n",
      "with client.scatter to reduce scheduler burden and \n",
      "keep data on workers\n",
      "\n",
      "    future = client.submit(func, big_data)    # bad\n",
      "\n",
      "    big_future = client.scatter(big_data)     # good\n",
      "    future = client.submit(func, big_future)  # good\n",
      "  % (format_bytes(len(b)), s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body counts computed 224.82254028320312\n",
      "body-counts done 227.7306730747223\n",
      "counting words body 250.6220347881317\n"
     ]
    }
   ],
   "source": [
    "body_counts = bodies_parsed.map_partitions(count_words).compute()\n",
    "now = time.time() - start_time\n",
    "print(f\"body counts computed {now}\")\n",
    "body_counts = sum(body_counts.tolist(), Counter())\n",
    "now = time.time() - start_time\n",
    "print(f\"body-counts done {now}\")\n",
    "title_counts = titles_parsed.map_partitions(count_words).compute()\n",
    "title_counts = sum(title_counts.tolist(), Counter())\n",
    "\n",
    "now = time.time() - start_time\n",
    "print(f\"counting words body {now}\")\n",
    "\n",
    "words_to_keep_body = body_counts.most_common(n=8000)\n",
    "body_vocab = defaultdict(lambda: 1)\n",
    "body_vocab.update({x:i+2 for i, x in enumerate([x[0] for x in words_to_keep_body])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting words title 250.68553686141968\n",
      "words counted 250.7349832057953\n",
      "saving 250.87703490257263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/dask/dataframe/core.py:2966: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('body', 'object'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n",
      "/usr/local/lib/python3.6/site-packages/dask/dataframe/core.py:2966: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('title', 'object'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    }
   ],
   "source": [
    "now = time.time() - start_time\n",
    "print(f\"counting words title {now}\")\n",
    "words_to_keep_title = title_counts.most_common(n=4500)\n",
    "titles_vocab = defaultdict(lambda: 1)\n",
    "titles_vocab.update({x:i+2 for i, x in enumerate([x[0] for x in words_to_keep_title])})\n",
    "\n",
    "now = time.time() - start_time\n",
    "print(f\"words counted {now}\")\n",
    "\n",
    "numer_bodies = bodies_parsed.apply(lambda x: [body_vocab[w] for w in x])\n",
    "numer_titles = titles_parsed.apply(lambda x: [titles_vocab[w] for w in x])\n",
    "\n",
    "def pad_partition(numerized_doc, max_len):\n",
    "    if type(numerized_doc) != list:\n",
    "        return\n",
    "    return pad_sequences([numerized_doc], maxlen=max_len, truncating='post')[0]\n",
    "\n",
    "processed_bodies = numer_bodies.apply(pad_partition, max_len=body_quant)\n",
    "processed_titles = numer_titles.apply(pad_partition, max_len=title_quant)\n",
    "\n",
    "now = time.time() - start_time\n",
    "print(f\"saving {now}\")\n",
    "\n",
    "processed_titles = np.stack(processed_titles.values.compute())\n",
    "processed_bodies = np.stack(processed_bodies.values.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating hdf5 376.64964151382446\n"
     ]
    }
   ],
   "source": [
    "now = time.time() - start_time\n",
    "print(f\"creating hdf5 {now}\")\n",
    "\n",
    "\n",
    "f = h5py.File('/data/dataset.hdf5', 'w')\n",
    "f.create_dataset('/titles', data=processed_titles)\n",
    "f.create_dataset('/bodies', data=processed_bodies)\n",
    "f.create_dataset('/targets', data=targets)\n",
    "f.close()\n",
    "\n",
    "with open(\"/data/metadata.json\", \"w\") as f:\n",
    "    meta = {\n",
    "        'body_vocab_size': len(body_vocab),\n",
    "        'title_vocab_size': len(titles_vocab),\n",
    "        'issue_body_doc_length': body_quant,\n",
    "        'issue_title_doc_length': title_quant,\n",
    "    }\n",
    "    f.write(json.dumps(meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved 382.772980928421\n"
     ]
    }
   ],
   "source": [
    "now = time.time() - start_time\n",
    "print(f\"saved {now}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
